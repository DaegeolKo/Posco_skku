<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>POSCO Industrial AI Robot Challenge 2025</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">POSCO Industrial AI Robot Challenge 2025</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">YoungWoo Son</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">HanSol Kang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">JaeYoung Oh</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">BumSu Yi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">DaegeolKo</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">SeongBo Ha</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">JaeHyuk Hur</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Robotics Innovatory, SKKU</span>
            <span class="author-block"><sup>2</sup>LAIR, SKKU</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span> -->
<!--                   <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/run_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!--<section class="section">
  <div class="container is-max-desktop">
      Abstract.
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p>
        </div>
      </div>
    </div>


    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. 
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">State Estimation</h2>

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <p>
          This module includes a <strong>SLAM system</strong> for building a <strong>3D point cloud map</strong> and a <strong>localization system for estimating the robot’s 6D pose</strong> on the pre-built map.<br>
          <p>Our SLAM and localization framework is based on  <strong>LIO (LiDAR-Inertial Odometry) with leg odometry</strong>, utilizing <strong>two LiDAR sensors</strong> and <strong>one IMU </strong>:</p>
          - <strong>LiDARs :</strong> Hesai XT16 and Unitree L1<br>
          &nbsp; &nbsp; &nbsp;The L1 LiDAR is used mainly for observing the ground surface, since in the competition environment the XT16 LiDAR alone &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;could not capture it reliably. <br>
          - <strong>IMU :</strong> The onboard IMU integrated in Unitree Go2 is used.


          </p>
          <center>
          <video id="teaser" autoplay muted loop playsinline width="600" >
            <source src="./static/videos/slam_demo.mp4"
                type="video/mp4">
          </video>
        </center>
        </div>
      </div>

    </div>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Planner</h2>

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Global Path Planner</h2>
          <p>
          Our global path planner is built on an occupancy grid map created by projecting the SLAM-based point cloud into 2D, combined with the <strong>A* algorithm</strong>. <br><br>
          In the competition setting, the robot only needs to navigate through designated corridors, and the safest path is always along the center of the corridor —<strong> where the robot maintains equal distance from obstacles</strong>. <br><br>
          To find this path, we calculate a <strong>signed distance field (SDF)</strong> from the occupancy grid map and derive its gradient. By masking the pixels near the zero-level gradient, we create what we call a zero-SDF gradient map. These masked pixels represent the central lines of corridors, equidistant from obstacles. We then <strong>restrict A* search to this reduced set of valid pixels</strong>. <br><br>
          This approach not only <strong>reduces the search space within narrow corridors</strong>, but also <strong>provides a high-quality initial guess for the local path planner</strong>, which subsequently refines the trajectory while considering detailed obstacle avoidance.<br>

          </p>
          <img src="./static/images/global_path_planner.png"
                class="a"
                alt="Interpolate start reference image."/>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Local Path Planner</h2>
        <div class="columns is-centered">
          <div class="column content">
            <br>
            <br>
            <br>
            <p>
              The local path planner is based on <strong>Model Predictive Control (MPC)</strong>. The robot’s collision model is represented by <strong>multiple collision barriers</strong>, and constraints are imposed to ensure that the <strong>SDF (Signed Distance Field)</strong> values at the centers of these barriers remain greater than a predefined safety margin, thereby enabling obstacle avoidance. For computational efficiency, the SDF is approximated using a Taylor expansion.<br><br>
              The planner <strong>optimizes H+1 future states and H velocity inputs to both follow the global path and smooth the control inputs</strong>. Among these, only the first velocity input is applied to the robot as the control command.
            </p>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            
            
            <img src="./static/images/local_path_planner.png"
                class="a"
                alt="Interpolate start reference image."/>
          </div>

        </div>
      </div>
    </div>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Control</h2>

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">System Architecture</h2>
          <p>
          To enable interaction with the low-level system using the policies and velocity commands that are generated from the Perception (SLAM, Path planning) stage to accomplish the required tasks.
          </p>
          <img src="./static/images/control_system_architecture.png"
                class="a"
                alt="Interpolate start reference image."/>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Control scheme</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <img src="./static/images/control_scheme.png"
                class="a"
                alt="Interpolate start reference image."/>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation.
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>


        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        / Interpolating.

        Re-rendering.
         <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> 


      </div>
    </div> -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">News</h2>

        <div class="content has-text-justified">
          <p>
            <a href="https://www.skku.edu/skku/campus/skk_comm/news.do?mode=view&articleNo=129911">성균관대, "우리 대학, 포스코홀딩스 ‘산업용 AI-로봇 경진대회’ 대상 수상", 2025.09.16</a>
          </p>
          <p>
            <a href="https://newsroom.posco.com/kr/%ed%8f%ac%ec%8a%a4%ec%bd%94%ed%99%80%eb%94%a9%ec%8a%a4-%ec%82%b0%ec%97%85%ec%9a%a9-ai-%eb%a1%9c%eb%b4%87-%ea%b2%bd%ec%a7%84%eb%8c%80%ed%9a%8c-%ea%b0%9c%ec%b5%9c-%ec%b2%ad%eb%85%84/">포스코그룹, "포스코홀딩스 ‘산업용 AI-로봇 경진대회’ 개최…청년 AI 전문가 육성에 힘 보탠다", 2025.09.10</a>
          </p>
          <p>
            <a href="https://www.hankyung.com/article/2025091537101">한국경제, "포스코홀딩스, 산업용 AI-로봇 경진대회…청년인재 육성", 2025.09.15</a>
          </p>
          <p>
            <a href="https://www.ytn.co.kr/_ln/0102_202509101626206108">YTN, "포스코홀딩스 '산업용 AI-로봇 경진대회' 개최", 2025.09.10</a>
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
